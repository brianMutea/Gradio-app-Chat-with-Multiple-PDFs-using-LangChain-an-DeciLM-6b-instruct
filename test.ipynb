{"cells":[{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["from pypdf import PdfReader\n","import torch\n","import PyPDF2\n","from io import BytesIO\n","from langchain.prompts import PromptTemplate\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.vectorstores import FAISS\n","from langchain.chains import RetrievalQA\n","import gradio as gr\n","import time\n","\n","from langchain.memory import ConversationBufferMemory\n","\n","\n","from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n","from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n","from langchain.document_loaders import PyPDFDirectoryLoader\n","\n","CHUNK_SIZE = 1000\n","# Using HuggingFaceEmbeddings with the chosen embedding model\n","embeddings = HuggingFaceEmbeddings(\n","    model_name=\"sentence-transformers/all-mpnet-base-v2\",model_kwargs = {\"device\": \"cuda\"})\n","\n","# transformer model configuration\n","quant_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","\n","def load_llm():\n","\n","    model_id = \"Deci/DeciLM-6b\"\n","    tokenizer = AutoTokenizer.from_pretrained(model_id)\n","    model = AutoModelForCausalLM.from_pretrained(model_id,\n","                                             trust_remote_code=True,\n","                                             device_map = \"auto\",\n","                                             quantization_config=quant_config)\n","    pipe = pipeline(\"text-generation\",\n","                    model=model,\n","                    tokenizer=tokenizer,\n","                    temperature=0,\n","                    num_beams=5,\n","                    no_repeat_ngram_size=4,\n","                    early_stopping=True,\n","                    max_new_tokens=50,\n","                )\n","    \n","    llm = HuggingFacePipeline(pipeline=pipe)\n","\n","    return llm"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["pdf_paths = \"/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs\"\n","\n","loader = PyPDFDirectoryLoader(\n","    path= pdf_paths,\n","    glob=\"*.pdf\"\n",")\n","documents=loader.load()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["[Document(page_content='Moûsai : Efficient Text-to-Music Diffusion Models\\nFlavio Schneider∗\\nETH Zürich\\nflavio.schneider.97@gmail.comOjasv Kamal∗\\nIIT Kharagpur\\nkamalojasv2000@gmail.com\\nZhijing Jin†\\nMPI for Intelligent Systems & ETH Zürich\\njinzhi@ethz.chBernhard Schölkopf†\\nMPI for Intelligent Systems\\nbs@tue.mpg.de\\nAbstract\\nRecent years have seen the rapid development\\nof large generative models for text; however,\\nmuch less research has explored the connec-\\ntion between text and another “language” of\\ncommunication – music . Music, much like text,\\ncan convey emotions, stories, and ideas, and\\nhas its own unique structure and syntax. In\\nour work, we bridge text and music via a text-\\nto-music generation model that is highly ef-\\nficient, expressive, and can handle long-term\\nstructure. Specifically, we develop Moûsai , a\\ncascading two-stage latent diffusion model that\\ncan generate multiple minutes of high-quality\\nstereo music at 48kHz from textual descrip-\\ntions. Moreover, our model features high effi-\\nciency, which enables real-time inference on a\\nsingle consumer GPU with a reasonable speed.\\nThrough experiments and property analyses,\\nwe show our model’s competence over a vari-\\nety of criteria compared with existing music\\ngeneration models. Lastly, to promote the open-\\nsource culture, we provide a collection of open-\\nsource libraries with the hope of facilitating\\nfuture work in the field.1\\n1 Introduction\\nIn recent years, natural language processing (NLP)\\nhas made significant strides in understanding and\\ngenerating human language, due to the advance-\\nments in deep learning and large-scale pre-trained\\nmodels (Radford et al., 2018; Devlin et al., 2019;\\nBrown et al., 2020). While the majority of NLP\\nresearch has focused on textual data, there exists\\nanother rich and expressive “language” of commu-\\nnication – music . Music, much like text, can convey\\nemotions (Germer, 2011), stories (Chung, 2006),\\nand ideas (Bicknell, 2002), and has its own unique\\nstructure and syntax (Swain, 1995).\\n∗Co-first author.†Co-supervision.\\n1We open-source the following:\\n– Codes: https://github.com/archinetai/audio-diffusion-pytorch\\n– Music samples for this paper: http://bit.ly/44ozWDH\\n– Music samples for all models: https://bit.ly/audio-diffusion\\nUNet 1Tokenizer\\nUNet 1UNet 1Text Description\\nNoise\\nNoiseAudioEmbedding\\nLatentTransformer\\nUNet 2UNet 2UNet 2UNet 2DiffusionDecoderDiffusionGeneratorTextEncoderEgyptian Darbuka, \\nDrums, R ythm, \\n(Deluxe Edition),\\n2 of 4Figure 1: We propose a two-stage cascading diffusion\\nmethod, where the first stage compresses the music\\nusing a novel diffusion autoencoder, and the second\\nstage generates music from the reduced representation\\nconditioned on the encoding of a textual description.\\nIn this paper, we further bridge the gap between\\ntext and music by leveraging the power of NLP\\ntechniques to generate music conditioned on tex-\\ntual input. Through our work, we not only aim\\nto expand the scope of NLP applications, but also\\ncontribute to the interdisciplinary research at the\\nintersection of language, music, and machine learn-\\ning techniques.\\nHowever, like text, music generation has long been\\na challenging task, as it requires multiple aspects\\nat different levels of abstraction (van den Oord\\net al., 2016; Dieleman et al., 2018). Existing au-\\ndio generation models explore the use of recursive\\nneural networks (Mehri et al., 2017), adversarial\\ngenerative networks (Kumar et al., 2019; Kim et al.,\\n2021; Engel et al., 2019; Morrison et al., 2022), au-\\ntoencoders (Deng et al., 2021), and transformers\\n(Yu et al., 2022a). With the recent advancement\\nin diffusion-based generative models in computerarXiv:2301.11757v3  [cs.CL]  23 Oct 2023', metadata={'source': '/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs/Text-to-Music Generation.pdf', 'page': 0}),\n"," Document(page_content='vision (Ramesh et al., 2022; Saharia et al., 2022),\\nresearchers in speech have also started to explore\\nthe use of diffusion models in tasks such as speech\\nsynthesis (Kong et al., 2021; Lam et al., 2022; Leng\\net al., 2022), although only a few these models can\\napply well to the task of music generation.\\nAdditionally, there are several long-standing chal-\\nlenges in the area of music generation: (1) music\\ngeneration at length, as most text-to-audio systems\\n(Forsgren and Martiros, 2022; Kreuk et al., 2022)\\ncan only generate a few seconds of audio; (2) model\\nefficiency, as many need to run on GPUs for hours\\nto generate just one minute of audio (Dhariwal\\net al., 2020; Kreuk et al., 2022); (3) lack of diver-\\nsity of the generated music, as many are limited by\\ntheir training methods taking in a single modality\\n(resulting in the ability to handle only single-genre\\nmusic, but not diverse genres) (Caillon and Esling,\\n2021; Pasini and Schlüter, 2022); and (4) easy con-\\ntrollability by text prompts, as most are only con-\\ntrolled by latent states (Caillon and Esling, 2021;\\nPasini and Schlüter, 2022), the starting snippet of\\nthe music (Borsos et al., 2022), or text but are lyrics\\n(Dhariwal et al., 2020) or descriptions of a daily\\nsound like dog barking (Kreuk et al., 2022).\\nA single model mastering all these aspects would\\nmake a strong contribution to the music industry,\\nas it can enable the broader public to be part of\\nthe creative process by allowing them to compose\\nmusic using an accessible text-based interface, as-\\nsist creators in finding inspiration, and provide an\\nunlimited supply of novel audio samples.\\nTo address these challenges, we propose Moûsai ,2\\na novel text-conditional two-stage cascading diffu-\\nsion model. Specifically, the first stage trains a mu-\\nsic encoder by diffusion magnitude-autoencoding\\n(DMAE), which compress audio by the novel dif-\\nfusion autoencoder; and the second stage learns to\\ngenerate the reduced representation while condi-\\ntioning on a textual description by text-conditioned\\nlatent diffusion (TCLD). The two-stage generation\\nprocess is shown in Figure 1.\\nApart from proposing the novel text-to-music diffu-\\nsion model, we also introduce some special designs\\nto boost model efficiency, making the model more\\naccessible. First, our DMAE can achieve an au-\\ndio signal compression rate of 64x. Moreover, we\\n2Moûsai is romanized ancient Greek for Muses , the sources\\nof artistic inspiration ( https://en.wikipedia.org/wiki/\\nMuses ), and also evokes a blend of music andAI.design a lightweight and specialized 1D U-Net ar-\\nchitecture. Together, our model achieves a fast\\ninference speed on a single consumer GPU in min-\\nutes, and a training time of approximately one week\\nper stage on one A100 GPU, making it possible\\nto train and run the overall system using resources\\navailable in most universities.\\nWe train our model on a newly collected dataset,\\nTEXT2MUSIC , with 50K text-music pairs covering\\ndiverse music genres. Remarkably, our diffusion-\\nbased model improves significantly on previous\\nmodels, as it can be trained on a variety of mu-\\nsic genres, generate long-context music for several\\nminutes with a high quality of 48kHz stereo music,\\nrunreal-time inference efficiently within minutes,\\nand can be easily controlled by text. Our extensive\\nevaluations on 11 criteria also validate the quality\\nof the generated music by our model from multi-\\nple perspectives, such as text-music relevance, and\\nmusic quality.\\nIn summary, our contributions are as follows:\\n1.We are the first to propose the text-to-music\\ndiffusion model using a two-stage cascading\\nlatent diffusion modeling process.\\n2.We achieve high efficiency with a compres-\\nsion rate of 64x, and a specialized U-Net de-\\nsign, which achieves a training time of one\\nweek on an A100 consumer GPU, and real-\\ntime inference time.\\n3.We collect TEXT2M USIC , a dataset of 50K\\ntext-music pairs constituting 2,500 hours of\\nmusic.\\n4.Our model outperforms existing baselines by\\nclear margins on 11 different evaluation cri-\\nteria, demonstrating merits such as high ef-\\nficiency, text-music relevance, music quality,\\nand long-context structure.\\n2 Related Work\\nConnecting Text and Music The connection be-\\ntween text and music lies in the intersection of NLP\\nand computational musicology. Previous work\\nlooks into aspects such as the similarity of mu-\\nsic and linguistic structures (Papadimitriou and Ju-\\nrafsky, 2020), music and dialog (Berlingerio and\\nBonin, 2018), and jointly modeling music and text\\nfor emotion detection (Mihalcea and Strapparava,\\n2012). Apart from several work that generates mu-\\nsic from text (Dhariwal et al., 2020; Forsgren and', metadata={'source': '/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs/Text-to-Music Generation.pdf', 'page': 1}),\n"," Document(page_content='Martiros, 2022), we are the first to explore diffusion\\nmodels to interact text with music representations.\\nGenerative Models Generative models aim to\\nlearn a lower-dimension representation space, and\\nthen reconstruct to the high-dimension space con-\\nditioning on the given information (Rombach et al.,\\n2022; Yang et al., 2022; Kreuk et al., 2022; Ho\\net al., 2022). Some effective methods earlier in-\\nclude auto-encoding (Hinton and Salakhutdinov,\\n2006; Kingma and Welling, 2014), or quantized\\nauto-encoding (van den Oord et al., 2017; Esser\\net al., 2021; Lee et al., 2022). Recent proposals\\nfocus on the quantized representation followed by\\nmasked or autoregressive learning on tokens (Vil-\\nlegas et al., 2022; Yu et al., 2022b; Chang et al.,\\n2023; Dhariwal et al., 2020; Borsos et al., 2022;\\nYang et al., 2022; Kreuk et al., 2022), and diffusion\\nmodels (Ramesh et al., 2022; Rombach et al., 2022;\\nSaharia et al., 2022; Ho et al., 2022; Forsgren and\\nMartiros, 2022), which leads to impressive perfor-\\nmance. To the best of our knowledge, we are the\\nfirst to adapt the cascading diffusion approach for\\naudio generation.\\n3 Moûsai: Efficient Long-Context Music\\nGeneration from Text\\nOur model Moûsai contains a two-stage training\\nprocess. In Stage 1, we use diffusion magnitude-\\nautoencoding (DMAE), which compresses the au-\\ndio waveform 64x using a diffusion autoencoder.\\nIn Stage 2, we use a latent text-to-audio diffusion\\nmodel, to generate a novel latent space by diffusion\\nwhile conditioning on text embeddings obtained\\nfrom a frozen transformer language model.\\n3.1 Stage 1: Music Encoding by Diffusion\\nMagnitude-Autoencoding (DMAE)\\nWe design the first step of Moûsai to be learning\\na good music encoder to capture the latent repre-\\nsentation space for music. Representation learn-\\ning is crucial for generative models, as it can be\\ndrastically more efficient than handling the high-\\ndimensional raw input data (Rombach et al., 2022;\\nYang et al., 2022; Kreuk et al., 2022; Ho et al.,\\n2022; Villegas et al., 2022).\\nOverview To learn the representation space for mu-\\nsic, we deploy a diffusion magnitude autoencoder\\n(DMAE) shown in Figure 2. Specifically, we adopt\\nour diffusion-based audio autoencoder, introduced\\nin Section 3.1.3, to compress audio into a smaller\\nUNet||·||NoiseEncoderSTFTMagLatent\\nAudio\\nFigure 2: The training scheme of our diffusion magni-\\ntude autoencoder (DMAE). When denoising (bottom\\nright), we condition the U-Net on the noise level ( ) and\\ncompressed latent representation ( ) from a reduced ver-\\nsion of the non-noisy audio (the pink matrix).\\nlatent space by 64x from the original waveform. To\\ntrain the model, we first convert the waveform to a\\nmagnitude spectrogram, which is a better represen-\\ntation for audio models, and then we auto-encode\\nit into a latent representation.\\nAt the same time, we corrupt the original audio with\\na random amount of noise, and train our 1D U-Net\\n(introduced in Section 3.1.4) to remove that noise.\\nDuring the noise removal process, we condition the\\nU-Net on the noise level and the compressed latent,\\nwhich can have access to a reduced version of the\\nnon-noisy audio.\\n3.1.1 vvv-Objective Diffusion\\nWe use the vvv-objective diffusion process as pro-\\nposed by Salimans and Ho (2022). Suppose we\\nhave a sample xxx0from a distribution p(xxx0), some\\nnoise schedule σt∈[0,1], and some noisy data\\npoint xxxσt=ασtxxx0+βσtϵϵϵ. The vvv-objective diffu-\\nsion tries to estimate a model ˆvvvσt=f(xxxσt, σt)by\\nminimizing the following objective:\\nEt∼[0,1],σt,xσt\\x02∥fθ(xσt, σt)−vσt∥2\\n2\\x03,(1)\\nwhere vvvσt=∂xxxσt\\nσt=ασtϵϵϵ−βσtxxx0, for which\\nwe define ϕt:=π\\n2σt, and obtain its trigonometric\\nvalues ασt:= cos( ϕt), and βσt:= sin( ϕt).\\n3.1.2 DDIM Sampler for Denoising\\nThe denoising step uses ODE samplers to turn noise\\ninto a new data point by estimating the rate of\\nchange. In this work, we adopt the DDIM sampler\\n(Song et al., 2021), which we find to work well', metadata={'source': '/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs/Text-to-Music Generation.pdf', 'page': 2}),\n"," Document(page_content='and have a reasonable tradeoff between the number\\nof steps and audio quality. The DDIM sampler\\ndenoises the signal by repeated application of the\\nfollowing:\\nˆvvvσt=fθ(xxxσt, σt) (2)\\nˆxxx0=ασtxxxσt−βσtˆvvvσt (3)\\nˆϵϵϵσt=βσtxxxσt+ασtˆvvvσt (4)\\nˆxxxσt−1=ασt−1ˆxxx0+βσt−1ˆϵϵϵt, (5)\\nwhich estimates both the initial data point and the\\nnoise at the step σt, for some T-step noise schedule\\nσT, . . . , σ 0as a sequence evenly spaced between 1\\nand 0.\\n3.1.3 Diffusion Autoencoder for Audio Input\\nWe propose a new diffusion autoencoder that first\\nencodes a magnitude spectrogram into a com-\\npressed representation, and later injects the latent\\ninto intermediate channels of the decoding mod-\\nules. The standard method to do diffusion, such as\\nthe image diffusion model (Rombach et al., 2022),\\nis to compress the input into a lower-dimensional\\nrepresentation space and apply the diffusion pro-\\ncess on the reduced latent space. We further com-\\npress and enhance the representation space by\\ndiffusion-based autoencoding (Preechakul et al.,\\n2022), which is first introduced in computer vision,\\nas a way to condition the diffusion process on a\\ncompressed latent vector of the input itself. Since\\ndiffusion serves as a more powerful generative de-\\ncoder, and hence the input can be reduced to latent\\nrepresentations with higher compression ratios.\\n3.1.4 Efficient and Enriched 1D U-Net\\nAnother crucial module in our model is the effi-\\ncient 1D U-Net that we design. We identify that\\nthe vanilla U-Net architecture (Ronneberger et al.,\\n2015), originally introduced for medial image seg-\\nmentation, has relatively limited efficiency and\\nspeed, as it uses an hourglass convolutional-only\\n2D architecture with skip connections.\\nHence, we propose a novel U-Net with only 1D\\nconvolutional kernels, which is more efficient than\\nthe original 2D architecture in terms of speed, and\\ncan be successfully used both on waveforms or on\\nspectrograms if each frequency is considered as a\\ndifferent channel.\\nMoreover, we infuse our 1D U-Net with multi-\\nple new components, as illustrated in Figure 3: a\\nResNet residual 1D convolutional unit, a modula-\\ntion unit to alter the channels given features from\\nDownsampleUpsampleItemsSkipUNetBlockItems\\nItems×NRCAMIFigure 3: Our proposed 1D U-Net architecture. Each\\nUNetBlock (top) consists of several U-Net items (bot-\\ntom). In each U-Net item (bottom), we use a 1D con-\\nvolutional ResNet (R), and a modulation unit (M) to\\nprovide the diffusion noise level as a feature vector con-\\nditioning ( ). For Stage 1, we use an inject item (I)\\nto inject external channels as conditioning ( ), and for\\nStage 2, we use an attention item (A) to share time-wise\\ninformation, and a cross-attention item (C) to condition\\non an external (text) embedding ( ). Moreover, for the\\nUNetBlock s, we can recursively nest them, which we\\nindicate by the inner dashed region on the top.\\nthe diffusion noise level, and an inject item to con-\\ncatenate external channels to the ones at the current\\ndepth. Note that inject items are applied only at a\\nspecific depth in the decoder in the first stage to\\ncondition on the latent representation of the music.\\nIn summary, our novel 1D U-Net features more\\nmodern convolutional blocks, a variety of attention\\nblocks, conditioning blocks, and improved skip\\nconnections, maintaining an efficient skeleton of\\nthe hourglass architecture.\\n3.1.5 Overall Model Architecture\\nOur entire Stage 1, DMAE, works as follows. Let\\nwwwbe a waveform of shape [c, t]forcchannels and t\\ntimesteps, and (mmmwww,pppwww) = stft( www;n= 1024 , h=\\n256) be the magnitude and phase obtained from a\\nshort-time furier tranform of the waveform with a\\nwindow size of 1024 and hop-length of 256. Then\\nthe resulting spectrograms will have shape [c·n,t\\nh].\\nWe discard phase and encode the magnitude into\\na latent zzz=Eθθθe(mmmwww)using a 1D convolutional\\nencoder. The original waveform is then recon-\\nstructed by decoding the latent using a diffusion\\nmodel ˆwww=Dθθθd(zzz,ϵϵϵ, s), where Dθθθdis the diffu-\\nsion sampling process with starting noise ϵϵϵands\\nis the number of decoding (sampling) steps. The\\ndecoder is trained with vvv-objective diffusion while\\nconditioning on the latent fθθθd(wwwσt;σt,zzz), where', metadata={'source': '/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs/Text-to-Music Generation.pdf', 'page': 3}),\n"," Document(page_content='UNet||·||NoiseText\\nEmbeddingEmbeddingTransformer\\nLatentFigure 4: The training scheme of our text-conditioned\\nlatent diffusion (TCLD) generator. During the denoising\\nprocess, we provide the U-Net a feature vector ( ) and\\na text embedding ( ).\\nfθθθdis the proposed 1D U-Net, called repeatedly\\nduring decoding.\\nSince only the magnitude is used and phase is\\ndiscarded, this diffusion autoencoder is simulta-\\nneously a compressing autoencoder and vocoder.\\nBy using the magnitude spectrograms, higher com-\\npression ratios can be obtained than autoencoding\\ndirectly the waveform. We found that waveforms\\nare less compressible and efficient to work with.\\nSimilarly, discarding phase is beneficial to obtain-\\ning higher compression ratios for the same level\\nof quality. The diffusion model can easily learn to\\ngenerate a waveform with realistic phase even if\\nconditioned only on the encoded magnitude.\\nIn this way, the latent space for music can serve\\nas the starting point for our text-to-music genera-\\ntor, which will be introduced next. To ensure this\\nrepresentation space fits the next stage, we apply a\\ntanh function on the bottleneck, keeping the val-\\nues in the range [−1,1]. Note that we do not use\\na more disentangled bottleneck, such as the one\\nin V AEs (Kingma and Welling, 2014), as its addi-\\ntional regularization reduces the amount of allowed\\ncompressibility.\\n3.2 Stage 2: Text-to-Music Generation by\\nText-Conditioned Latent Diffusion\\n(TCLD)\\nBased on the learned music representation space,\\nin this stage, we guide the music generation with\\ntext descriptions.\\nOverview As shown in Figure 4, we propose a\\ntext-conditioned latent diffusion (TCLD) process.\\nSpecifically, we first corrupt the latent space of\\nmusic with a random amount of noise, then train a\\nseries of U-Nets to remove the noise, and conditionthe U-Nets’ denoising process on a text prompt\\nencoded by a transformer model. In this way, the\\ngenerated music both conforms to the latent space\\nof music and corresponds to the text prompt.\\n3.2.1 Text Conditioning\\nTo obtain the text embeddings, prior work on text-\\nconditioning suggests either learning a joint data-\\ntext representation (Li et al., 2022; Elizalde et al.,\\n2022; Ramesh et al., 2022), or using embeddings\\nfrom pre-trained language model as direct condi-\\ntioning (Saharia et al., 2022; Ho et al., 2022) of the\\nlatent model. In our TCLD model, we follow the\\npractice in Saharia et al. (2022) to use a pre-trained\\nand frozen T5 language model (Raffel et al., 2020)\\nto generate text embeddings from the given descrip-\\ntion. We use the classifier-free guidance (CFG) (Ho\\nand Salimans, 2022) with a learned mask applied\\non batch elements with a probability of 0.1 to im-\\nprove the strength of the text-embedding during\\ninference.\\n3.2.2 Adapting the U-Net for Text\\nConditioning\\nTo enable the U-Net to condition on the text em-\\nbedding eee, we append two additional blocks to\\nthe U-Net: an attention item to share long-context\\nstructural information, and a cross-attention item\\nto condition on the text embeddings, as in Figure 3.\\nThese attention blocks ensure information sharing\\nover the entire latent space, which is crucial to learn\\nlong-range audio structure.\\nGiven the compressed size of the latent space, we\\nalso increase the size of this inner U-Net to be\\nlarger than the first stage. And due to our efficiency\\ndesign, it maintains a reasonable training and infer-\\nence speed, even with large parameter counts.\\n3.2.3 Overall Model Architecture\\nWe illustrate the detailed process in Figure 4. Con-\\nsistent with the previous stage, we use vvv-objective\\ndiffusion and the 1D U-Net architecture. When con-\\ndition on the text embedding eee, we use the U-Net\\nconfiguration fθθθg(zzzσt;σt,eee)to generate the com-\\npressed latent zzz=Eθθθe(mmmwww). Then, the generator\\nGθθθg(eee,ϵϵϵ, s)applies DDIM sampling and calls the\\nU-Net stimes to generate an approximate latent ˆzzz\\nfrom the text embedding eeeand starting noise ϵϵϵ. The\\nfinal generation stack during inference to obtain a\\nwaveform is\\nˆwww=Dθθθd(Gθθθg(eee,ϵϵϵg, sg),ϵϵϵd, sd). (6)', metadata={'source': '/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs/Text-to-Music Generation.pdf', 'page': 4}),\n"," Document(page_content='4 Experimental Setup\\n4.1 Collection of the T EXT2M USIC Dataset\\nTo provide a fertile ground to train our text-\\nto-music model on, we collect a new dataset,\\nTEXT2M USIC , which consists of 50K text-music\\npairs totaling 2,500 hours. We ensure a high qual-\\nity of stereo music sampled at 48kHz and cover\\na wide variety of music spanning multiple genres,\\nartists, instruments, and provenience. Many ex-\\nisting open-source music datasets, such as Gillick\\net al. (2019); Hawthorne et al. (2019a), have limi-\\ntations in terms of the specific musical instruments\\nthey encompass. While some datasets, like Engel\\net al. (2017); Boulanger-Lewandowski et al. (2012),\\ncover a broader array of instruments, they fall short\\nin representing a wide variety of genres. This in-\\nadequacy underscores the need for a more compre-\\nhensive dataset that encompasses a rich tapestry of\\nmusical genres and diverse instrumentation.\\nAs for the procedure to collect the music, we first\\ncheck with the copyright regulations, which grants\\nan exemption for using copyright infringing copies\\nif the purpose is scientific research (Geiger et al.,\\n2018; Delacroix, 2023), according to the EU regu-\\nlation in Article 3 of the EU Directive on Copyright\\nin the Digital Single Market (European Commis-\\nsion, 2016). Then, we follow Spotify’s top rec-\\nommendations to collect seven very large playlists,\\neach containing on average 7K pieces of music.\\nWe iterate through every music sample in these\\nplaylists, for which we use the name of the music\\nto search and download the music from YouTube,\\nand we use the metadata to compose its correspond-\\ning text description, which contains the music title,\\nauthor, album name, genre, and year of release.\\nIn line with our spirit to open-source the model,\\nwe also open-source the data collection pipeline\\non GitHub,3so future researchers can use it to\\nfacilitate new data collection.\\nWe show the statistics about the diverse set of gen-\\nres in our T EXT2MUSIC dataset in Table 1.\\n4.2 Implementation Details\\nOur diffusion autoencoder has 185M parame-\\nters, and text-conditional generator has 857M pa-\\nrameters, with more architecture details in Ap-\\npendix A.3. We train the music autoencoder on\\n3https://github.com/archinetai/\\naudio-data-pytorchGenre # Pieces Percentage (%) in Dataset\\nPop 5,498 27.29\\nElectronic 3,875 19.38\\nRock 3,584 17.79\\nMetal 1,796 8.92\\nHip Hop 818 4.06\\nOthers 4,492 22.56\\nTable 1: Our TEXT2MUSIC dataset covers a variety of\\nmusic, e.g., pop, electronic, rock, metal, hip pop, etc.\\nrandom crops of length 218(∼5.5s at 48kHz), and\\nthe text-conditional diffusion generation model on\\nfixed crops of length 221(∼44s at 48kHz) encoded\\nin the 32-channels, 64x compressed latent represen-\\ntation. We use the AdamW optimizer (Loshchilov\\nand Hutter, 2019) with a learning rate of 10−4,β1\\nof0.95,β2of0.999,ϵof10−6, and weight de-\\ncay of 10−3. And we use an exponential moving\\naverage (EMA) with β= 0.995and power of 0.7.\\n5 Evaluation\\n5.1 Assessment Criteria Overview\\nEvaluating music is a highly challenging task. We\\nsurvey a large number of papers, and find that pre-\\nvious work adopts a variety of objective and subjec-\\ntive metrics,4and the gist is that no single metric is\\nperfect. After careful thinking, we design a com-\\nprehensive set of evaluation metrics covering three\\ncategories with a total of 11 metrics , including both\\nautomatic and human evaluations. In the following,\\nwe will introduce the overall property analysis (Sec-\\ntion 5.2), such as the sample rate, prompt type, and\\nmusic type; efficiency (Section 5.3); text-music rel-\\nevance (Section 5.4); music quality (Section 5.5);\\nand long-term structure of the music (Section 5.6).\\n5.2 Property Analysis\\nComparing the overall properties of various models\\nin Table 2, we see a set of impressive properties\\nof the Moûsai model: (1) We are among the very\\nfew that can control music generation easily by text\\ndescriptions of the type of music we want, as most\\nother models do not take text as input (van den\\nOord et al., 2016; Caillon and Esling, 2021; Borsos\\net al., 2022), or take only lyrics or descriptions of\\ndaily sounds (e.g., “a dog barking”) (Kreuk et al.,\\n4The common metrics we surveyed include quality (Goel\\net al., 2022), fidelity (Goel et al., 2022; Hawthorne et al.,\\n2019b; Hyun et al., 2022), musicality (Goel et al., 2022; Yu\\net al., 2022a; Dhariwal et al., 2020), diversity (Goel et al.,\\n2022; Dhariwal et al., 2020), and structure (Yu et al., 2022a;\\nLeng et al., 2022; Dhariwal et al., 2020).', metadata={'source': '/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs/Text-to-Music Generation.pdf', 'page': 5}),\n"," Document(page_content='Model Sample Rate ↑Len.↑Input (Text ✓) Music (Diverse ↑) Example Infer. Time ↓Data\\nWaveNet (2016) 16kHz@1 Secs None Piano or speech Piano = Audio len.⋆260\\nJukebox (2020) 44.1kHz@1 Mins⋆Lyrics , author, etc. Song with the lyrics Song Hours 70K\\nRA VE (2021) 48kHz@2 Secs⋆Latent Single-genre Music Strings = Audio len.⋆100\\nAudioLM (2022) 16kHz@1 Secs⋆Beginning of the music Piano or speech Piano Mins 40K\\nMusika (2022) 22.5kHz@2 Secs Context vector Single-genre Music Piano = Audio len.⋆1K\\nRiffusion (2022) 44.1kHz@1 5s Text (genre, author, etc.) Music of any genre Jazzy clarinet Mins –\\nAudioGen (2022) 16kHz@1 Secs⋆Text (a phrase/sentence) Daily sounds Dog barks Hours 4K\\nMoûsai (Ours) 48kHz @2 Mins⋆Text (genre, author, etc.) Music of any genre African drums = Audio len. 2.5K\\nTable 2: Comparison of our Moûsai model with previous music/audio generation models. We compare the followings\\naspects: (1) audio sample rate@the number of channels (Sample Rate ↑, where the higher the better), (2) context\\nlength of the generated music (Len. ↑, where the higher the more capable the model is to generate structural music;⋆\\nindicates variable length, where we assume that autoregressive methods are variable by default, with an upper-bound\\nimposed by attention), (3) input type (Input, where we feature using Text as the condition for the generation), (4)\\ntype of the generate music (Music, where the more Diverse ↑genre, the better), (5) an example of the generated\\nmusic type (Example), (6) inference time (Infer. Time ↓, where the shorter the better, and since the music length is\\nseconds or minutes, the inference time equivalent to the audio length is the shortest, and we use⋆to show models\\nthat can run inference fast on CPU), and (7) total length of the music in the training data in hours (Data).\\n2022; Dhariwal et al., 2020). The only other text-\\nto-music model is the Riffusion model (Forsgren\\nand Martiros, 2022), which only works with very\\nshort length of 5 seconds.\\n(2) Our model is also among the very few that\\nenables long-context music generation for several\\nminutes, among all others that can only gener-\\nate seconds (van den Oord et al., 2016; Forsgren\\nand Martiros, 2022; Kreuk et al., 2022; Pasini\\nand Schlüter, 2022), except for Jukebox (Dhari-\\nwal et al., 2020) which generates songs given lyrics\\nand takes very long to run inference.\\n(3) Moreover, we also highlight the diversity of\\nmusic we generate, as our model design enables\\nmulti-genre music training, instead of single-genre\\nones in previous models (Caillon and Esling, 2021;\\nPasini and Schlüter, 2022), and we can find rhythm,\\nloops, riffs, and occasionally even entire choruses\\nin our generated music.\\n5.3 Efficiency of Our Model\\nEfficiency is another highlight of our model, where\\nwe only need an inference time similar to the audio\\nlength on a consumer GPU, which is several min-\\nutes, while many other text-to-audio models take\\nmany GPU hours (Dhariwal et al., 2020; Kreuk\\net al., 2022), as in Table 2. Our model is very\\nfriendly for research at university labs, as each\\nmodel can be trained on a single A100 GPU in 1\\nweek of training using a batch size of 32. This is\\nequivalent to around 1M steps for both the diffusion\\nautoencoder and latent generator. For inference, as\\nan example, a novel audio source of ∼43s can besynthesized in less than 50s using a consumer GPU\\nwith a DDIM sampler and a high step count (100\\ngeneration steps and 100 decoding steps).\\nWe also calculate the exact inference statistics for\\nour Moûsai vs. Riffusion models in Table 4, and\\nfind that our model needs less than 1/5 the inference\\ntime, and almost half of the inference memory than\\nRiffusion does.\\nModel Inf. Time (s) ( ↓) Mem. (G) ( ↓) RTF (↓)\\nRiffusion 218.0 8.85 5.07\\nMoûsai 49.2 5.04 1.14\\nTable 3: Efficiency evaluation of our Moûsai and Rif-\\nfusion in terms of the inference time (Inf. Time) by\\nseconds, inference memory (Mem.) by Gigabytes , and\\nthe real-time factor (RTF) to generate a single 43-second\\nmusic clip.\\n5.4 Evaluating the Text-Music Relevance\\nTo assess how much the generated music corre-\\nsponds to the given text prompt, we deploy both\\nhuman and automatic evaluations.\\nRelevance & Distinctiveness by Human Evalua-\\ntion We design a listener test where the annotators\\nneed to infer some coarse information of the text\\nprompt behind a given piece of generated music.\\nSince it is too challenging to infer the exact text\\nprompt, we only ask annotators to infer the music\\ngenre indicated in the prompt.\\nTo prepare the ground-truth prompts, we com-\\npose a list of 40 random text prompts spanning\\nacross the four most common music genres in our', metadata={'source': '/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs/Text-to-Music Generation.pdf', 'page': 6}),\n"," Document(page_content='Electronic Hip Hop Metal PopElectronic\\nHip Hop\\nMetal\\nPop\\n051015202530\\n(a) Confusion matrix for the\\nmusic pieces generated by\\nMoûsai. ( y-axis: true genre;\\nx-axis: inferred genre.)\\nElectronic Hip Hop Metal PopElectronic\\nHip Hop\\nMetal\\nPop\\n051015202530\\n(b) Confusion matrix for the\\nmusic pieces generated by\\nthe Riffusion model.\\nFigure 5: For the text-music relevance check, we ask\\nthe annotators to infer the ground-truth genres of the\\ngenerated music by (a) our model and (b) the Riffusion\\nmodel. The darker diagonal means better results. Note\\nthat each matrix adds up to 120, corresponding to 40\\nsamples per model annotated by three perceivers each.\\nTEXT2MUSIC dataset: electronic, hip hop, metal,\\nand pop. See Appendix C.1 for the entire list of\\nprompts. Inspired by the two-alternative forced\\nchoice (2AFC) experiment design, we design a\\nfour-alternative forced choice (4AFC) paradigm,\\nwhere the annotators need to categorize each mu-\\nsic sample into exactly one of the four provided\\ncategories. See annotation details in Appendix C.1.\\nIn Figure 5, we can see that our Moûsai model has\\nthe most mass on the diagonal (i.e., correctly iden-\\ntified), while the Riffusion model tends to generate\\ngeneric samples that are mostly identified as pop\\nfor all ground-truth genres. This shows that the\\nmusic generated by our model is both relevant to\\nthe test and distinct enough with the given genre\\nagainst others.\\nRelevance by CLAP For automatic evaluation, we\\nadopt the commonly used CLAP score (Wu et al.,\\n2023) to quantify the alignment between the gen-\\nerated audio and the corresponding text. From\\nTable 4, we can see that our model is two times\\nbetter than Riffusion in terms of the CLAP score,\\nand also much faster in inference time. Specifically,\\nwe apply the pretrained CLAP model (Wu et al.,\\n2023)5to get the embeddings of each music sample\\nand its corresponding text prompt, and then report\\ntheir cosine similarity as the CLAP score.\\nModel CLAP Score for Text-Music Relevance ( ↑)\\nRiffusion 0.06\\nMoûsai 0.13\\nTable 4: CLAP scores of our Moûsai and Riffusion.\\n5https://github.com/LAION-AI/CLAP5.5 Evaluating the Music Quality\\nWe first introduce the four evaluation metrics for\\nmusic quality, and then describe the results.\\n5.5.1 Metrics for Music Quality\\nTo evaluate the quality of the generated music, we\\nadopt four metrics: the automatic score by FAD, a\\nmusic Turing test, and human evaluation on musi-\\ncality and audio clarity.\\nFor automatic evaluation, we deploy the widely\\nadopted Fréchet Audio Distance (FAD) (Kilgour\\net al., 2019) to assess the fidelity of the generated\\nmusic distribution in comparison to the real music\\ndistribution (i.e., how similar the generated music\\nis to the authentic music). To facilitate the com-\\nputation of FAD, we employ the commonly used\\nPANN model (Kong et al., 2020)6as a means to\\neffectively encode the music.\\nThen, we also set up three human evaluations, all\\non a scale of 1 (the worst) to 5 (the best). First,\\nwe let human annotators to assess the authentic-\\nity/fidelity of the generated music via a music Tur-\\ning test (Goel et al., 2022; Hawthorne et al., 2019b;\\nHyun et al., 2022). Specifically, we ask the an-\\nnotators to listen to a pair of music samples at a\\ntime, and judge which one is real and which is\\ngenerated. To provide a more fine-grained score,\\nwe also ask them how much the generated music\\nthey identified sounds like real music, on a scale\\nof 1 (not similar at all) to 5 (highly similar). We\\nkeep their annotation score if they identify the gen-\\nerated music correctly, and otherwise we rate the\\nmusic as 5, which means that the music perfectly\\npasses the Turing test. See more evaluation details\\nin Appendix C.2.\\nThe other two metrics we deploy are musicality and\\naudio clarity . For musicality, we let human anno-\\ntators rate the melodiousness and harmoniousness\\n(Seitz, 2005) of the given music. And for audio\\nclarity, or quality (Goel et al., 2022), we let them\\njudge how close the quality is to a walkie-talkie\\n(worst) or a high-quality studio sound system (best).\\nThe detailed setup of all our human evaluations are\\nin Appendix C.2 and Appendix C.3.\\n5.5.2 Results\\nWe show the evaluation results on all five metrics\\nin Table 5. We can see that, on the automatic evalu-\\n6https://github.com/gudgud96/\\nfrechet-audio-distance', metadata={'source': '/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs/Text-to-Music Generation.pdf', 'page': 7}),\n"," Document(page_content='ation of FAD, our model has the best score, which\\nis one magnitude smaller than previous models.\\nMoreover, it also shows strong performance across\\nthe human evaluation metrics, outperforming the\\nother two models on the music Turing test, har-\\nmoniousness, and sound clarity, as well as being\\ncomparable on the melodiousness metric.\\nModel FAD ( ↓) Fidelity Melody Harmony Clarity\\nRiffusion 0.0018 2.8 2.66 2.48 2.37\\nMusika 0.0020 3.04 3.21 3.04 2.88\\nMoûsai 0.00015 3.17 3.15 3.08 2.92\\nTable 5: Music quality scores for the three models.\\n5.6 Long-Term Structure of the Music\\n2 4 6 8 10 12 14 16\\nSubsegment0.00.10.20.3Average Amplitude\\nAverage Amplitude of various segments\\n1 of 4\\n2 of 4\\n3 of 4\\n4 of 4\\nFigure 6: The average amplitude and variation of 1K\\nrandom music samples spanning different segments.\\nIn music composition, the arrangement of a piece\\ntypically follows a gradual introduction, a main\\nbody with the core content, and a gradual conclu-\\nsion, also called the sonata form (Webster, 2001).\\nAccordingly, we look into whether our generated\\nmusic also shows such a long-term structure. Us-\\ning the same text prompt, we can generate different\\nsegments/intervals of it by attaching the expression\\n“1/2/3/4 out of 4” at the end of the text prompt, such\\nas “Italian Hip Hop 2022, 3 of 4.” We randomly\\ngenerate 1,000 music pieces, where the prompts\\nare from a uniform distribution of the four segment\\ntags. We visualize the results in Figure 6, where\\nwe see the first segment shows a gradual increase\\nin both the average amplitude and variance, fol-\\nlowed by continuously high average amplitude and\\nvariance throughout Segments 2 and 3, and finally\\nconcluding with a gradual decline in the last seg-\\nment.\\n5.7 Effect of Hyperparameters\\nWe also explore the effect of different hyperparam-\\neters, and find that increasing the number of atten-\\ntion blocks (e.g., from a total of 4–8 to a total of\\n32+) in the latent diffusion model can improve the\\ngeneral structure of the songs, thanks to the long-\\ncontext view. Also, if the model is trained withoutattention blocks, the context provided by the U-\\nNet is not large enough to learn any meaningful\\nlong-term structure. We describe other variations\\nof hyperparameters and findings in Appendix D.\\n6 Conclusion\\nIn this work, we presented Moûsai, a novel text-\\nto-music generation model using latent diffusion.\\nWe show that, in contrast to earlier approaches,\\nour model can generate minutes of music in real-\\ntime on a consumer GPU, with good music quality\\nand text-audio binding. In addition, we provide\\na collection of open-source libraries to facilitate\\nfuture work in the field. The work helps pave the\\nway towards higher-quality, longer-context text-to-\\nmusic generation for future applications.\\nLimitations and Future Work\\nData Scale Enhancing the scale of both data and\\nthe model holds promising potential for yielding\\nsignificant improvements in quality. Following\\n(Dhariwal et al., 2020; Borsos et al., 2022), we\\nsuggest training with 50K-100K hours instead of\\n2.5K. Computer Vision studies like Saharia et al.\\n(2022) show that utilizing larger pretrained lan-\\nguage models for text embeddings plays an im-\\nportant role in achieving better quality outcomes.\\nDrawing upon this, we hypothesize that the ap-\\nplication of a larger pretrained language model to\\nour second-stage model can similarly contribute to\\nenhanced quality outcomes.\\nModels Some promising future modelling ap-\\nproaches that can be explored in future work in-\\nclude: (1) training diffusion models using percep-\\ntual losses on the waveforms instead of L2 — this\\nmight help decrease the initial size of the U-Net,\\nas we would not have to process non-perceivable\\nsounds, (2) improving the quality of the diffusion\\nautoencoder by using mel-spectrograms instead of\\nmagnitude spectrograms as input, (3) other types of\\nconditioning which are not text-based might be use-\\nful to navigate the audio latent space, which is often\\nhard to describe in words — DreamBooth-like mod-\\nels (Ruiz et al., 2022), and (4) more sophisticated\\ndiffusion samplers to achieve higher quality for the\\nsame number of sampling steps, or similarly more\\nadvanced distillation techniques (Salimans and Ho,\\n2022).', metadata={'source': '/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs/Text-to-Music Generation.pdf', 'page': 8}),\n"," Document(page_content='Author Contributions\\nFlavio Schneider came up with the idea, made\\nmany architecture innovations, trained the model,\\nand wrote a significant portion of the paper. This\\nmodel is one of the several models he proposed as\\npart of his Master’s thesis at ETH Zürich (Schnei-\\nder, 2023).\\nOjasv Kamal deployed our models on the clas-\\nsical music data, set up the baseline models, and\\nconducted most of the analyses for the automatic\\nevaluation and human evaluation.\\nZhijing Jin co-supervised this work and Flavio’s\\nMaster’s thesis, conducted weekly meetings,\\nhelped designed the structure of the paper, led a set\\nof human evaluation of the models, and contributed\\nsignificantly to the writing.\\nBernhard Schölkopf supervised the work and pro-\\nvided precious suggestions during the design pro-\\ncess of this work, as well as extensive suggestions\\non the writing.\\nAcknowledgment\\nOur project would not have been made possible\\nwithout the GPU resources from various parties:\\nWe thank Shehzaad Dhuliawala for helping us set\\nup GPU access at ETH Zürich. We thank Vincent\\nBerenz and Lidia Pavel for setting up our GPU ac-\\ncess at Max Planck Institute. We thank Stability\\nAI for their generous support for the computational\\nresources for our first version of the model. We\\nappreciate the writing help from Rada Mihalcea to\\npolish this paper with the idea that text and music\\nare both a form of language. We are also grateful\\nfor the generous help by our annotators including\\nYuen Chen, Andrew Lee, Aylin Gunal, Fernando\\nGonzalez, and Yiwen Ding. We thank Nasim Ra-\\nhaman for early-stage discussions to improve the\\nmodel design and contributions. We thank Fer-\\nnando Gonzalez and Zhiheng Lyu for helping to\\nimprove the format of the paper.\\nThis material is based in part upon works sup-\\nported by the German Federal Ministry of Edu-\\ncation and Research (BMBF): Tübingen AI Cen-\\nter, FKZ: 01IS18039B; and by the Machine Learn-\\ning Cluster of Excellence, EXC number 2064/1\\n– Project number 390727645. Zhijing Jin is sup-\\nported by PhD fellowships from the Future of Life\\nInstitute and Open Philanthropy, as well as thetravel support from ELISE (GA no 951847) for the\\nELLIS program.\\nEthical Considerations\\nOur work aims to bridge the gap between text and\\nmusic generation, enabling the creation of expres-\\nsive and high-quality music from textual descrip-\\ntions. While this research has the potential to ben-\\nefit various applications, such as music therapy,\\nentertainment, and education, we recognize that\\nit may also raise concerns in terms of copyright,\\ncultural appropriation, and the potential misuse of\\ngenerated content.\\nCopyright and Intellectual Property: Our model\\nmay generate music that resembles existing copy-\\nrighted works, which could lead to potential legal\\ndisputes. First of all, for research-only use, it is\\nexempted from copyright infringement, as we men-\\ntioned in the data collection section previously. For\\nother purposes, we suggest incorporating mecha-\\nnisms to detect and avoid generating music that\\nclosely resembles copyrighted material.\\nEconomic Impact on Musicians and Composers:\\nThe widespread adoption of text-to-music genera-\\ntion models may have economic implications for\\nmusicians and composers, potentially affecting\\ntheir livelihoods. We believe that our model should\\nbe used as a tool to augment and inspire human\\ncreativity, rather than replace it. We encourage col-\\nlaboration between AI researchers, musicians, and\\ncomposers to explore new ways of integrating AI-\\ngenerated music into the creative process, ensuring\\nthat the technology benefits all stakeholders.\\nIn conclusion, we are committed to conducting\\nour research responsibly and ethically. We encour-\\nage the research community to engage in open dis-\\ncussions about the ethical implications of text-to-\\nmusic generation models and to develop guidelines\\nand best practices for their responsible use. By\\naddressing these concerns, we hope to contribute\\nto the development of AI technologies that benefit\\nsociety and promote creativity, while respecting the\\nrights and values of all stakeholders.\\nReferences\\nBBC Music Magazine. 2022. Classical music: 50 great-\\nest composers of all time. BBC Music Magazine.\\nMichele Berlingerio and Francesca Bonin. 2018. To-\\nwards a music-language mapping. In Proceedings of', metadata={'source': '/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs/Text-to-Music Generation.pdf', 'page': 9}),\n"," Document(page_content='the Eleventh International Conference on Language Re-\\nsources and Evaluation (LREC 2018) , Miyazaki, Japan.\\nEuropean Language Resources Association (ELRA).\\nJeanette Bicknell. 2002. Can music convey semantic\\ncontent? a kantian approach. The Journal of Aesthetics\\nand Art Criticism , 60(3):253–261.\\nZalán Borsos, Raphaël Marinier, Damien Vincent,\\nEugene Kharitonov, Olivier Pietquin, Matthew Shar-\\nifi, Olivier Teboul, David Grangier, Marco Tagliasac-\\nchi, and Neil Zeghidour. 2022. AudioLM: A lan-\\nguage modeling approach to audio generation. CoRR ,\\nabs/2209.03143.\\nNicolas Boulanger-Lewandowski, Yoshua Bengio, and\\nPascal Vincent. 2012. Modeling temporal dependencies\\nin high-dimensional sequences: Application to poly-\\nphonic music generation and transcription.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen\\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh,\\nDaniel Ziegler, Jeffrey Wu, Clemens Winter, Chris\\nHesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\\nGray, Benjamin Chess, Jack Clark, Christopher Berner,\\nSam McCandlish, Alec Radford, Ilya Sutskever, and\\nDario Amodei. 2020. Language models are few-shot\\nlearners. In Advances in Neural Information Processing\\nSystems , volume 33, pages 1877–1901. Curran Asso-\\nciates, Inc.\\nAntoine Caillon and Philippe Esling. 2021. RA VE: A\\nvariational autoencoder for fast and high-quality neural\\naudio synthesis. CoRR , abs/2111.05011.\\nHuiwen Chang, Han Zhang, Jarred Barber, Aaron\\nMaschinot, José Lezama, Lu Jiang, Ming-Hsuan Yang,\\nKevin Murphy, William T. Freeman, Michael Rubin-\\nstein, Yuanzhen Li, and Dilip Krishnan. 2023. Muse:\\nText-to-image generation via masked generative trans-\\nformers. CoRR , abs/2301.00704.\\nSheng-Kuan Chung. 2006. Digital storytelling in inte-\\ngrated arts education. The International Journal of Arts\\nEducation , 4(1):33–50.\\nSylvie Delacroix. 2023. Data rivers: Carving out the\\npublic domain in the age of Chat-GPT. Available at\\nSSRN .\\nKangle Deng, Aayush Bansal, and Deva Ramanan. 2021.\\nUnsupervised audiovisual synthesis via exemplar au-\\ntoencoders. In 9th International Conference on Learn-\\ning Representations, ICLR 2021, Virtual Event, Austria,\\nMay 3-7, 2021 . OpenReview.net.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of deep\\nbidirectional transformers for language understanding.\\nInProceedings of the 2019 Conference of the North\\nAmerican Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Volume 1\\n(Long and Short Papers) , pages 4171–4186, Minneapo-lis, Minnesota. Association for Computational Linguis-\\ntics.\\nPrafulla Dhariwal, Heewoo Jun, Christine Payne,\\nJong Wook Kim, Alec Radford, and Ilya Sutskever.\\n2020. Jukebox: A generative model for music. CoRR ,\\nabs/2005.00341.\\nSander Dieleman, Aäron van den Oord, and Karen Si-\\nmonyan. 2018. The challenge of realistic music gen-\\neration: Modelling raw audio at scale. In Advances\\nin Neural Information Processing Systems 31: Annual\\nConference on Neural Information Processing Systems\\n2018, NeurIPS 2018, December 3-8, 2018, Montréal,\\nCanada , pages 8000–8010.\\nBenjamin Elizalde, Soham Deshmukh, Mahmoud Al\\nIsmail, and Huaming Wang. 2022. CLAP: learning au-\\ndio concepts from natural language supervision. CoRR ,\\nabs/2206.04769.\\nJesse Engel, Cinjon Resnick, Adam Roberts, Sander\\nDieleman, Douglas Eck, Karen Simonyan, and Moham-\\nmad Norouzi. 2017. Neural audio synthesis of musical\\nnotes with wavenet autoencoders.\\nJesse H. Engel, Kumar Krishna Agrawal, Shuo Chen,\\nIshaan Gulrajani, Chris Donahue, and Adam Roberts.\\n2019. Gansynth: Adversarial neural audio synthesis.\\nIn7th International Conference on Learning Represen-\\ntations, ICLR 2019, New Orleans, LA, USA, May 6-9,\\n2019 . OpenReview.net.\\nPatrick Esser, Robin Rombach, and Björn Ommer. 2021.\\nTaming transformers for high-resolution image synthe-\\nsis. In IEEE Conference on Computer Vision and Pat-\\ntern Recognition, CVPR 2021, virtual, June 19-25, 2021 ,\\npages 12873–12883. Computer Vision Foundation /\\nIEEE.\\nEuropean Commission. 2016. Proposal for a directive of\\nthe European parliament and of the council on copyright\\nin the digital single market.\\nSeth* Forsgren and Hayk* Martiros. 2022. Riffusion -\\nStable diffusion for real-time music generation.\\nChristophe Geiger, Giancarlo Frosio, and Oleksandr Bu-\\nlayenko. 2018. The exception for text and data mining\\n(tdm) in the proposed directive on copyright in the digi-\\ntal single market-legal aspects. Centre for International\\nIntellectual Property Studies (CEIPI) Research Paper ,\\n(2018-02).\\nMark Germer. 2011. Notes , 67(4):760–765.\\nDaniel Gillick, Sayali Kulkarni, Larry Lansing, Alessan-\\ndro Presta, Jason Baldridge, Eugene Ie, and Diego\\nGarcia-Olano. 2019. Learning dense representations for\\nentity retrieval. In Computational Natural Language\\nLearning (CoNLL) .\\nKaran Goel, Albert Gu, Chris Donahue, and Christopher\\nRé. 2022. It’s raw! audio generation with state-space\\nmodels. In International Conference on Machine Learn-\\ning, ICML 2022, 17-23 July 2022, Baltimore, Maryland,\\nUSA, volume 162 of Proceedings of Machine Learning\\nResearch , pages 7616–7633. PMLR.', metadata={'source': '/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs/Text-to-Music Generation.pdf', 'page': 10}),\n"," Document(page_content='Gal Greshler, Tamar Rott Shaham, and Tomer Michaeli.\\n2021. Catch-a-waveform: Learning to generate audio\\nfrom a single short example. In Advances in Neural\\nInformation Processing Systems 34: Annual Confer-\\nence on Neural Information Processing Systems 2021,\\nNeurIPS 2021, December 6-14, 2021, virtual , pages\\n20916–20928.\\nCurtis Hawthorne, Andriy Stasyuk, Adam Roberts, Ian\\nSimon, Cheng-Zhi Anna Huang, Sander Dieleman,\\nErich Elsen, Jesse Engel, and Douglas Eck. 2019a. En-\\nabling factorized piano music modeling and generation\\nwith the MAESTRO dataset. In International Confer-\\nence on Learning Representations .\\nCurtis Hawthorne, Andriy Stasyuk, Adam Roberts, Ian\\nSimon, Cheng-Zhi Anna Huang, Sander Dieleman,\\nErich Elsen, Jesse H. Engel, and Douglas Eck. 2019b.\\nEnabling factorized piano music modeling and gen-\\neration with the MAESTRO dataset. In 7th Interna-\\ntional Conference on Learning Representations, ICLR\\n2019, New Orleans, LA, USA, May 6-9, 2019 . OpenRe-\\nview.net.\\nGeoffrey E Hinton and Ruslan R Salakhutdinov. 2006.\\nReducing the dimensionality of data with neural net-\\nworks. science , 313(5786):504–507.\\nJonathan Ho, William Chan, Chitwan Saharia, Jay\\nWhang, Ruiqi Gao, Alexey A. Gritsenko, Diederik P.\\nKingma, Ben Poole, Mohammad Norouzi, David J.\\nFleet, and Tim Salimans. 2022. Imagen video: High def-\\ninition video generation with diffusion models. CoRR ,\\nabs/2210.02303.\\nJonathan Ho and Tim Salimans. 2022. Classifier-free\\ndiffusion guidance. CoRR , abs/2207.12598.\\nLee Hyun, Taehyun Kim, Hyolim Kang, Minjoo Ki,\\nHyeonchan Hwang, Kwanho Park, Sharang Han, and\\nSeon Joo Kim. 2022. Commu: Dataset for combinato-\\nrial music generation. CoRR , abs/2211.09385.\\nKevin Kilgour, Mauricio Zuluaga, Dominik Roblek,\\nand Matthew Sharifi. 2019. Fréchet audio distance: A\\nmetric for evaluating music enhancement algorithms.\\nMinsu Kim, Joanna Hong, and Yong Man Ro. 2021.\\nLip to speech synthesis with visual context attentional\\nGAN. In Advances in Neural Information Processing\\nSystems 34: Annual Conference on Neural Information\\nProcessing Systems 2021, NeurIPS 2021, December\\n6-14, 2021, virtual , pages 2758–2770.\\nDiederik P. Kingma and Max Welling. 2014. Auto-\\nencoding variational bayes. In 2nd International Con-\\nference on Learning Representations, ICLR 2014, Banff,\\nAB, Canada, April 14-16, 2014, Conference Track Pro-\\nceedings .\\nQiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,\\nWenwu Wang, and Mark D. Plumbley. 2020. Panns:\\nLarge-scale pretrained audio neural networks for audio\\npattern recognition.\\nZhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and\\nBryan Catanzaro. 2021. Diffwave: A versatile diffusionmodel for audio synthesis. In 9th International Confer-\\nence on Learning Representations, ICLR 2021, Virtual\\nEvent, Austria, May 3-7, 2021 . OpenReview.net.\\nFelix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel\\nSinger, Alexandre Défossez, Jade Copet, Devi Parikh,\\nYaniv Taigman, and Yossi Adi. 2022. AudioGen: Tex-\\ntually guided audio generation. CoRR , abs/2209.15352.\\nKundan Kumar, Rithesh Kumar, Thibault de Boissiere,\\nLucas Gestin, Wei Zhen Teoh, Jose Sotelo, Alexandre\\nde Brébisson, Yoshua Bengio, and Aaron C. Courville.\\n2019. Melgan: Generative adversarial networks for con-\\nditional waveform synthesis. In Advances in Neural\\nInformation Processing Systems 32: Annual Confer-\\nence on Neural Information Processing Systems 2019,\\nNeurIPS 2019, December 8-14, 2019, Vancouver, BC,\\nCanada , pages 14881–14892.\\nMax W. Y . Lam, Jun Wang, Dan Su, and Dong Yu.\\n2022. BDDM: bilateral denoising diffusion models for\\nfast and high-quality speech synthesis. In The Tenth\\nInternational Conference on Learning Representations,\\nICLR 2022, Virtual Event, April 25-29, 2022 . OpenRe-\\nview.net.\\nDoyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho,\\nand Wook-Shin Han. 2022. Autoregressive image gen-\\neration using residual quantization. In IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition,\\nCVPR 2022, New Orleans, LA, USA, June 18-24, 2022 ,\\npages 11513–11522. IEEE.\\nYichong Leng, Zehua Chen, Junliang Guo, Haohe Liu,\\nJiawei Chen, Xu Tan, Danilo P. Mandic, Lei He, Xiang-\\nYang Li, Tao Qin, Sheng Zhao, and Tie-Yan Liu. 2022.\\nBinauralgrad: A two-stage conditional diffusion prob-\\nabilistic model for binaural audio synthesis. CoRR ,\\nabs/2205.14807.\\nManling Li, Ruochen Xu, Shuohang Wang, Luowei\\nZhou, Xudong Lin, Chenguang Zhu, Michael Zeng,\\nHeng Ji, and Shih-Fu Chang. 2022. Clip-event: Con-\\nnecting text and images with event structures. In\\nIEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition, CVPR 2022, New Orleans, LA, USA,\\nJune 18-24, 2022 , pages 16399–16408. IEEE.\\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\\nweight decay regularization. In 7th International Con-\\nference on Learning Representations, ICLR 2019, New\\nOrleans, LA, USA, May 6-9, 2019 . OpenReview.net.\\nSoroush Mehri, Kundan Kumar, Ishaan Gulrajani,\\nRithesh Kumar, Shubham Jain, Jose Sotelo, Aaron C.\\nCourville, and Yoshua Bengio. 2017. SampleRNN: An\\nunconditional end-to-end neural audio generation model.\\nIn5th International Conference on Learning Represen-\\ntations, ICLR 2017, Toulon, France, April 24-26, 2017,\\nConference Track Proceedings . OpenReview.net.\\nRada Mihalcea and Carlo Strapparava. 2012. Lyrics,\\nmusic, and emotions. In Proceedings of the 2012\\nJoint Conference on Empirical Methods in Natural\\nLanguage Processing and Computational Natural Lan-\\nguage Learning , pages 590–599, Jeju Island, Korea.\\nAssociation for Computational Linguistics.', metadata={'source': '/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs/Text-to-Music Generation.pdf', 'page': 11}),\n"," Document(page_content='Max Morrison, Rithesh Kumar, Kundan Kumar, Prem\\nSeetharaman, Aaron C. Courville, and Yoshua Bengio.\\n2022. Chunked autoregressive GAN for conditional\\nwaveform synthesis. In The Tenth International Confer-\\nence on Learning Representations, ICLR 2022, Virtual\\nEvent, April 25-29, 2022 . OpenReview.net.\\nIsabel Papadimitriou and Dan Jurafsky. 2020. Learning\\nMusic Helps You Read: Using transfer to study linguis-\\ntic structure in language models. In Proceedings of the\\n2020 Conference on Empirical Methods in Natural Lan-\\nguage Processing (EMNLP) , pages 6829–6839, Online.\\nAssociation for Computational Linguistics.\\nMarco Pasini and Jan Schlüter. 2022. Musika!\\nfast infinite waveform music generation. CoRR ,\\nabs/2208.08706.\\nKonpat Preechakul, Nattanat Chatthee, Suttisak Wizad-\\nwongsa, and Supasorn Suwajanakorn. 2022. Diffusion\\nautoencoders: Toward a meaningful and decodable rep-\\nresentation. In IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, CVPR 2022, New Or-\\nleans, LA, USA, June 18-24, 2022 , pages 10609–10619.\\nIEEE.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\\nIlya Sutskever. 2018. Improving language understand-\\ning by generative pre-training. Technical report, Ope-\\nnAI.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\\nLi, and Peter J. Liu. 2020. Exploring the limits of\\ntransfer learning with a unified text-to-text transformer.\\nJ. Mach. Learn. Res. , 21:140:1–140:67.\\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\\nChu, and Mark Chen. 2022. Hierarchical text-\\nconditional image generation with CLIP latents. CoRR ,\\nabs/2204.06125.\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz,\\nPatrick Esser, and Björn Ommer. 2022. High-resolution\\nimage synthesis with latent diffusion models. In\\nIEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition, CVPR 2022, New Orleans, LA, USA,\\nJune 18-24, 2022 , pages 10674–10685. IEEE.\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox.\\n2015. U-net: Convolutional networks for biomedical\\nimage segmentation. In Medical Image Computing and\\nComputer-Assisted Intervention - MICCAI 2015 - 18th\\nInternational Conference Munich, Germany, October 5\\n- 9, 2015, Proceedings, Part III , volume 9351 of Lecture\\nNotes in Computer Science , pages 234–241. Springer.\\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\\nMichael Rubinstein, and Kfir Aberman. 2022. Dream-\\nbooth: Fine tuning text-to-image diffusion models for\\nsubject-driven generation. ArXiv , abs/2208.12242.\\nChitwan Saharia, William Chan, Saurabh Saxena, Lala\\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\\nGhasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,\\nRapha Gontijo Lopes, Tim Salimans, Jonathan Ho,David J. Fleet, and Mohammad Norouzi. 2022. Photo-\\nrealistic text-to-image diffusion models with deep lan-\\nguage understanding. CoRR , abs/2205.11487.\\nTim Salimans and Jonathan Ho. 2022. Progressive dis-\\ntillation for fast sampling of diffusion models. In The\\nTenth International Conference on Learning Represen-\\ntations, ICLR 2022, Virtual Event, April 25-29, 2022 .\\nOpenReview.net.\\nFlavio Schneider. 2023. ArchiSound: Audio generation\\nwith diffusion.\\nJay A Seitz. 2005. Dalcroze, the body, movement and\\nmusicality. Psychology of music , 33(4):419–435.\\nJiaming Song, Chenlin Meng, and Stefano Ermon. 2021.\\nDenoising diffusion implicit models. In 9th Interna-\\ntional Conference on Learning Representations, ICLR\\n2021, Virtual Event, Austria, May 3-7, 2021 . OpenRe-\\nview.net.\\nJoseph P Swain. 1995. The concept of musical syntax.\\nThe Musical Quarterly , 79(2):281–308.\\nAlan M. Turing. 1950. I.—COMPUTING MACHIN-\\nERY AND INTELLIGENCE. Mind , LIX(236):433–\\n460.\\nAäron van den Oord, Sander Dieleman, Heiga\\nZen, Karen Simonyan, Oriol Vinyals, Alex Graves,\\nNal Kalchbrenner, Andrew W. Senior, and Koray\\nKavukcuoglu. 2016. Wavenet: A generative model for\\nraw audio. In The 9th ISCA Speech Synthesis Workshop,\\nSunnyvale, CA, USA, 13-15 September 2016 , page 125.\\nISCA.\\nAäron van den Oord, Oriol Vinyals, and Koray\\nKavukcuoglu. 2017. Neural discrete representation\\nlearning. In Advances in Neural Information Processing\\nSystems 30: Annual Conference on Neural Information\\nProcessing Systems 2017, December 4-9, 2017, Long\\nBeach, CA, USA , pages 6306–6315.\\nRuben Villegas, Mohammad Babaeizadeh, Pieter-Jan\\nKindermans, Hernan Moraldo, Han Zhang, Moham-\\nmad Taghi Saffar, Santiago Castro, Julius Kunze, and\\nDumitru Erhan. 2022. Phenaki: Variable length\\nvideo generation from open domain textual description.\\nCoRR , abs/2210.02399.\\nJames Webster. 2001. Sonata form. The new Grove\\ndictionary of music and musicians , 23:687–698.\\nYusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Tay-\\nlor Berg-Kirkpatrick, and Shlomo Dubnov. 2023. Large-\\nscale contrastive language-audio pretraining with fea-\\nture fusion and keyword-to-caption augmentation.\\nDongchao Yang, Jianwei Yu, Helin Wang, Wen Wang,\\nChao Weng, Yuexian Zou, and Dong Yu. 2022. Diff-\\nsound: Discrete diffusion model for text-to-sound gen-\\neration. CoRR , abs/2207.09983.\\nBotao Yu, Peiling Lu, Rui Wang, Wei Hu, Xu Tan, Wei\\nYe, Shikun Zhang, Tao Qin, and Tie-Yan Liu. 2022a.\\nMuseformer: Transformer with fine- and coarse-grained\\nattention for music generation. CoRR , abs/2210.10349.', metadata={'source': '/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs/Text-to-Music Generation.pdf', 'page': 12}),\n"," Document(page_content='Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong,\\nGunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander\\nKu, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson,\\nWei Han, Zarana Parekh, Xin Li, Han Zhang, Jason\\nBaldridge, and Yonghui Wu. 2022b. Scaling autoregres-\\nsive models for content-rich text-to-image generation.\\nCoRR , abs/2206.10789.', metadata={'source': '/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs/Text-to-Music Generation.pdf', 'page': 13}),\n"," Document(page_content='A More Data Details\\nA.1 Data Collection Rationale\\nWe have several desiderata when collecting the\\ndataset. The data must (1) have text data paired\\nwith the music piece, and (2) consistitute a large\\nsize, which means that our data crawling procedure\\nneeds to be scalable, without tedious manual efforts\\nto curate. Note that it is crucial to get a large-sized\\ndataset in order to unleash the performance of audio\\ngeneration diffusion models.\\nA.2 Training Setup for the Text-Music Pairs\\nFor the textual description, we use metadata such\\nas the title, author, album, genre, and year of re-\\nlease. Given that a song could span longer than\\n44s, we append a string indicating which chunk is\\ncurrently being trained on, together with the total\\nchunks the song is made of (e.g., 1 of 4 ). This\\nallows to select the region of interest during infer-\\nence. Hence, an example prompt is like “Egyptian\\nDarbuka, Drums, Rythm, (Deluxe Edition), 2 of 4. ”\\nTo make the conditioning more robust, we shuffle\\nthe list of metadata and drop each element with a\\nprobability of 0.1. Furthermore, for 50% of the\\ntimes we concatenate the list with spaces and the\\nother 50% of the times we use commas to make\\nthe interface more robust during inference. Some\\nexample prompts in our dataset can be seen in Ta-\\nble 6.\\nExample Text Prompts in Our Dataset\\nNr. 415 (Premium Edition), german hip hop, 2 of 7, 2012,\\nXATAR, Konnekt\\n30 Años de Exitos, Mundanzas, 2 of 6, latin pop, Lupita\\nD’Alessio, 2011\\nemo rap 2018 Runaway Lil Peep 4 of 5\\nAlone, Pt. II (Remixes) 2020 electro house Alone, Pt. II -\\nDa Tweekaz Remix Alan Walker\\nTable 6: Example text prompts in our dataset.\\nA.3 Model Architecture and Parameters\\nOur diffusion autoencoder has 185M parameters,\\nwith 7 nested U-Net blocks of increasing channel\\ncount ([256, 512, 512, 512, 1024, 1024, 1024]), for\\nwhich we downsample each time by 2, except for\\nthe first block ([1, 2, 2, 2, 2, 2, 2]). This makes the\\ncompression factor for our autoencoder to be 64x.\\nDepending on the desired speed/quality tradeoff,\\nmore or less compression can be applied in this\\nfirst stage. Following our single GPU constraint,\\nwe find that 64x compression factor is a good bal-\\nance to make sure the second stage can work ona reduced representation. We discuss more about\\nthis tradeoff in Appendix D.5. The diffusion au-\\ntoencoder only uses ResNet and modulation items\\nwith the repetitions [1, 2, 2, 2, 2, 2, 2]. We do not\\nuse attention, to allow decoding of variable and\\npossibly very long latent representations. Channel\\ninjection only happens at depth 4, which matches\\nthe output of the magnitude encoder latent, after\\napplying the tanh function.\\nOur text-conditional generator has 857M parame-\\nters (including the parameters of the frozen T5-base\\nmodel) with 6 nested U-Net blocks of increasing\\nchannel counts ([128, 256, 512, 512, 1024, 1024]),\\nand again downsampling each time by 2, except for\\nthe first block ([1, 2, 2, 2, 2, 2]). We use attention\\nblocks at the depths [0, 0, 1, 1, 1, 1], skipping the\\nfirst two blocks to allow for further downsampling\\nbefore sharing information over the entire latent,\\ninstead use cross-attention blocks at all resolutions\\n([1, 1, 1, 1, 1, 1]). For both attention and cross-\\nattention, we use 64 head features and 12 heads per\\nlayer. We repeat items with an increasing count\\ntowards the inner U-Net low-resolution and large-\\ncontext blocks ([2, 2, 2, 4, 8, 8]), this allows good\\nstructural learning over minutes of audio.\\nB More Experimental Details\\nB.1 Hardware Requirements\\nWe use limited computational resources as avail-\\nable in a university lab. Efficiency is a highlight\\nof our model, where we only need an inference\\ntime equivalent to the audio length on a consumer\\nGPU, which is several minutes, while many other\\ntext-to-audio models take many GPU hours (Dhari-\\nwal et al., 2020; Kreuk et al., 2022). Our model\\nis very friendly for research at university labs, as\\neach of our models can be trained on a single A100\\nGPU in 1 week of training using a batch size of\\n32; this is equivalent to around 1M steps for both\\nthe diffusion autoencoder and latent generator. For\\ninference, as an example, a novel audio source of\\n∼43s can be synthesized in less than 50s using a\\nconsumer GPU with a DDIM sampler and a high\\nstep count (100 generation steps and 100 decoding\\nsteps).', metadata={'source': '/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs/Text-to-Music Generation.pdf', 'page': 14}),\n"," Document(page_content='C More Evaluation Details\\nC.1 Annotation Details for the Genre\\nIdentification Test\\nPrompts We design a listener test to illustrate the\\ndiversity and text relevance of Moûsai. Specifi-\\ncally, we compose a list of 40 text prompts span-\\nning across the four most common music genres\\nin our dataset: electronic, hip hop, metal, and pop.\\nThe four genres are the most prevalent ones in our\\nTEXT2MUSIC dataset, which is collected from var-\\nious top playlists on Spotify. Among the 50K music\\nsamples, we identify the genre of them and report\\nthe distribution of genre in Table 1. The four genres\\nthat we select for the evaluation are top in the data\\ndistribution of different genres.\\nWhen composing the test samples, we also make\\nefforts to ensure comprehensive coverage. We first\\nread through the text samples in the dataset and\\nthen compose samples that are reasonable music\\ndescriptions but do not exist in the data. The text\\nprompts comprehensively cover all genres that we\\nevaluate and incorporate essential metadata such\\nas song title, album, artist, and year. Combining\\nthese elements allows us to generate diverse and\\ncomprehensive test prompts. We list all the text\\nprompts composed for the four common music\\ngenres in Table 7.\\nUsing these prompts, we generate music with both\\nMoûsai and the Riffusion model (Forsgren and Mar-\\ntiros, 2022), with a total of 80 pieces of music, two\\nfor each prompt.\\nAnnotation To validate this quantitatively, we con-\\nducted a listener test with three perceivers (annota-\\ntors) with diverse demographic backgrounds (both\\nfemale and male, all with at least a Bachelor’s de-\\ngree of education). Each annotator listens to all\\n80 music samples we provide, and is instructed to\\ncategorize each sample into exactly one of the four\\nprovided genres.\\nWe record how many times the perceiver correctly\\nidentifies the genre which the respective model was\\ngenerating from. A large number (or score) means\\nthat the model often generated music that, accord-\\ning to the human perceiver, plausibly belonged to\\nthe correct category (when compared to the other\\nthree categories). To achieve a good score, the\\nmodel needs to generate diverse and genre-specific\\nmusic. We take the score as a quality score ofthe model when it comes to correctly performing\\ntext-conditional music generation.\\nC.2 Annotation Details for Turing Test\\nWe conduct an evaluation employing an experiment\\nwith a similar spirit to the Turing test (Turing, 1950)\\nfor natural language, but commonly called as the\\nfidelity test in audio evaluation (Hyun et al., 2022)\\nor speaker test (Greshler et al., 2021; Hawthorne\\net al., 2019b) in audio evaluation.\\nWe let the annotators listen to a pair of music sam-\\nples at a time, and judge which one is real and\\nwhich is generated. To provide a more fine-grained\\nscore, we also ask them how much the generated\\nmusic they identified sounds like real music, on a\\nscale of 1 (almost not similar at all) to 5 (highly\\nsimilar). We keep their annotation score if they\\nidentify the generated music correctly, and other-\\nwise we rate the music as 5, which means that the\\nmusic perfectly passes the Turing test.\\nAs for the details, we create 90 music samples, in-\\ncluding 15 generated samples paired with 15 real\\nmusic samples for each of the three models (Rif-\\nfusion, Musika, and Moûsai). We recruit two un-\\ndergraduate annotators who have pursued playing\\nmusic as a hobby for the past 10 years. The an-\\nnotators were compensated with 500 rupees ( ∼6.5\\ndollars) for this 3 hour task (which is well above\\ndaily minimum wage in India).\\nWe presented a group of expert annotators with a to-\\ntal of 60 distinct folders, 15 corresponding to each\\nof Mousai, Mousai (classical-only), Riffusion, and\\nMusika models. Each folder contains two music\\nfiles, one being the original and the other generated\\nusing a given model prompted with its correspond-\\ning metadata.\\nFollowing are the exact instructions provided to the\\nannotators:\\n1.You will be presented with batches of two au-\\ndio samples in subfolders of this folder named\\nfrom 1 to 60. Each subfolder contains two\\naudios named a.wav and b.wav.\\n2. Listen to each sample carefully.\\n3.It’s best to use headphones in a quiet environ-\\nment if you can.\\n4.Some files may be loud, so it’s recommended\\nto keep the volume moderate.\\n5.One of the audio samples in each pair is a\\nreal recording, while the other is a generated', metadata={'source': '/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs/Text-to-Music Generation.pdf', 'page': 15}),\n"," Document(page_content='(synthetic) audio.\\n6.Listen to each pair of audio samples carefully.\\n7.Pay attention to the quality, characteristics,\\nand nuances of each audio sample.\\n8.This folder contains a spreadsheet file called\\n‘Response_Task_2.xlsx’. Compare the sam-\\nples to each other and provide a relative rating\\nto the fake audio only out of 5, where 1 being\\nthe most fake and 5 being most real.\\nC.3 Annotation Details for Musicality\\nIn order to ascertain the quality and artistic merit\\nof the generated musical output, we conduct a hu-\\nman evaluation. First, we prepare a total of 50\\nfolders, each containing three distinct audio files,\\nand present them to the human evaluators. We de-\\nsign the prompts in Table 8 , and run our model\\nand all the baseline models on them. We recruit\\ntwo undergraduate annotators in India, who have\\npursued playing music as a hobby for the past 10\\nyears. The annotators were compensated with 500\\nrupees ( ∼6.5 dollars) for this 3 hour task (which is\\nwell above daily minimum wage in India).\\nFollowing are the exact instructions provided to the\\nannotators:\\n1.Listen to the music and rate it based on three\\naspects: Quality, Melody, and Harmony.\\n2.It’s best to use headphones in a quiet environ-\\nment if you can.\\n3.Some files may be loud, so it’s recommended\\nto keep the volume moderate.\\n4.This folder contains folders subfolders\\nthrough 1-50. Each subfolders contains three\\naudio files named A.wav, B.wav, and C.wav.\\nYou need to listen to each of them and rate\\nthem (relative to each other) based on quality,\\nmelody, and harmony.\\n5.For Quality, consider how clear the audio\\nsounds. Does it resemble a walkie-talkie (bad\\nquality) or a high-quality studio sound system\\n(good quality)?\\n6.Melodiousness refers to the main pitch or note\\nin the music. Pay attention to the rhythm and\\nrepetitiveness of the melody. A more rhyth-\\nmic and repetitive melody is considered better,\\nwhile the opposite is true for a less rhythmic\\nmelody.\\n7.Harmoniousness involves multiple notes\\nplayed together to support the melody. Evalu-\\nate if these notes are in sync and enhance the\\nFigure 7: Mel spectrogram comparison between the true\\nsamples (top) and the auto-encoded samples (bottom);\\ncf. text.\\neffect of the melody. Higher scores should be\\ngiven for good harmony and lower for poor\\nharmony.\\n8.It is recommended view youtube videos: this\\nor this short video explaining melody and har-\\nmony\\n9.This folder also contains a spreadsheet by the\\nname “Response_Task_1.xlsx”. Remember\\nto provide ratings (out of 5) for each aspect\\nof your evaluation in the file against appropri-\\nate folder number. Feel free to listen to each\\nsample as many times before rating them.\\nD Exploring Variations of the Model\\nArchitecture and Training Setup\\nD.1 High-Frequency Sounds\\nWe observe that our model is good at handling\\nlow-frequency sounds. From the mel spectrograms\\nFigure 7, and also the music samples we provide,\\nwe notice that our model performs well with drum-\\nlike sounds as frequently found in electronic, house,\\ndubstep, techno, EDM, and metal music. This is\\nlikely a consequence of the lower amount of infor-\\nmation required to represent low-frequency sounds.\\nD.2 Improving the Structure\\nWe find that increasing the number of attention\\nblocks (e.g., from a total of 4 – 8 to a total of\\n32+) in the latent diffusion model can improve\\nthe general structure of the songs, thanks to the\\nlong-context view. If the model is trained without\\nattention blocks, the context provided by the U-\\nNet is not large enough to learn any meaningful\\nlong-term structure.', metadata={'source': '/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs/Text-to-Music Generation.pdf', 'page': 16}),\n"," Document(page_content='D.3 Text-Audio Binding\\nWe find that the text-audio binding works well with\\nCFG higher than 3.0. Since the model is trained\\nwith metadata such as title, album, artist, genre,\\nyear, and chunk, the best keywords to control the\\ngeneration appear to be frequent descriptive names,\\nsuch as the genre of the music, or descriptions com-\\nmonly found in titles, such as “remix” ,“(Deluxe\\nEdition)” , and possibly many more. A similar be-\\nhavior has been observed and exploited in text-to-\\nimage models to generate better looking results.\\nD.4 Trade-Off between Speed and Quality\\nWe find that 10 sampling steps in both stages can\\nbe enough to generate reasonable audio. We can\\nachieve improved quality and reduced noise for\\nhigh-frequency sounds by trading off the speed,\\ni.e., increasing the number of sampling steps in\\nthe diffusion decoder, e.g., 50 – 100 steps) will\\nsimilarly improve the quality, likely due to the more\\ndetailed generated latents, and at the same time\\nresult in an overall better structured music. To\\nmake sure the results are comparable when varying\\nthe number of sampling steps, we use the same\\nstarting noise in both stages. In both cases, this\\nsuggests that using more advanced samplers could\\nbe helpful to improve on the speed-quality trade-\\noff.\\nD.5 Trade-Off between Compression Ratio\\nand Quality\\nWe find that decreasing the compression ratio of\\nthe first stage (e.g., to 32x) can improve the qual-\\nity of low-frequency sounds, but in turn will slow\\ndown the model, as the second stage has to work\\non higher dimensional data. As proposed later in\\nSection 6, we hypothesize that using perceptually\\nweighted loss functions instead of L2 loss during\\ndiffusion could help this trade-off, giving a more\\nbalanced importance to high frequency sounds even\\nat high compression ratios.\\nD.6 High-Frequency Audio Generation\\nWe have encountered challenges in achieving satis-\\nfactory results when dealing with high-frequency\\naudio signals, as detailed in Appendix D.1. To\\ngain deeper insights into the underlying issues, we\\nconducted an ablation experiment by exclusively\\ntraining our model on classical music, a genre\\nknown for its prominent high-frequency charac-\\nteristics. We train this model using 500 hours ofmusic collected from albums of top classical com-\\nposers (BBC Music Magazine, 2022) and other\\npopular Spotify playlists. We notice a drop of 9.5%\\nin the fidelity score of the generated music samples\\ncompared to those produced by our original model.\\nFurther, qualitative analysis reveals that melodic\\nelements of these samples demonstrated commend-\\nable accuracy, the harmony notes appeared to be\\nconvoluted and disorganized. This finding high-\\nlights the significance of harmonization challenges\\nwhen generating high-frequency audio and under-\\nscores the need for developing improved models in\\nfuture research.', metadata={'source': '/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs/Text-to-Music Generation.pdf', 'page': 17}),\n"," Document(page_content='Genre = Electronic\\n– Drops, Kanine Remix, Darkzy, Drops Remixes, bass\\nhouse, (Deluxe) (Remix) 3 of 4\\n– Electronic, Dance, EDM (Deluxe) (Remix) 3 of 4\\n– Electro House (Remix), 2023, 3 of 4\\n– Electro Swing Remix 2030 (Deluxe Edition) 3 of 4\\n– Future Bass, EDM (Remix) 3 of 4, Remix\\n– EDM (Deluxe) (Remix) 3 of 4\\n– EDM, V ocal, Relax, Remix, 2023, 8D Audio\\n– Hardstyle, Drop, 8D, Remix, High Quality, 2 of 4\\n– Dubstep Insane Drop Remix (Deluxe Edition), 2 of 4\\n– Drop, French 79, BPM Artist, V ol. 4, Electronica, 2016\\nGenre = Hip Hop\\n– Real Hip Hop, 2012, Lil B, Gods Father, escape room, 3\\nof 4\\n– C’est toujours pour ceux qui savent, French Hip Hop,\\n2018 (Deluxe), 3 of 4\\n– Dejando Claro, Latin Hip Hop 2022 (Deluxe Edition) 3\\nof 4\\n– Latin Hip Hop 2022 (Deluxe Edition) 3 of 4\\n– Alternative Hip Hop Oh-My, 2016, (Deluxe), 3 of 4\\n– Es Geht Mir Gut, German Hip Hop, 2016, (Deluxe), 3 of\\n4\\n– Italian Hip Hop 2022 (Deluxe Edition) 3 of 4\\n– RUN, Alternative Hip Hop, 2016, (Deluxe), 3 of 4\\n– Hip Hop, Rap Battle, 2018 (High Quality) (Deluxe Edi-\\ntion) 3 of 4\\n– Hip Hop Tech, Bandlez, Hot Pursuit, brostep, 3 of 4\\nGenre = Metal\\n– Death Metal, 2012, 3 of 4\\n– Heavy Death Metal (Deluxe Edition), 3 of 4\\n– Black Alternative Metal, The Pick of Death (Deluxe),\\n2006, 3 of 4\\n– Kill For Metal, Iron Fire, To The Grave, melodic metal, 3\\nof 4\\n– Melodic Metal, Iron Dust (Deluxe), 2006, 3 of 4\\n– Possessed Death Metal Stones (Deluxe), 2006, 3 of 4\\n– Black Metal Venom, 2006, 3 of 4\\n– The Heavy Death Metal War (Deluxe), 2006, 3 of 4\\n– Heavy metal (Deluxe Edition), 3 of 4\\n– Viking Heavy Death Metal (Deluxe), 2006, 3 of 4\\nGenre = Pop\\n– (Everything I Do), I Do It For You, Bryan Adams, The\\nBest Of Me, canadian pop, 3 of 4\\n– Payphone, Maroon 5, Overexposed, Pop, 2021, 3 of 4\\n– 24K Magic, Bruno Mars, 24K Magic, dance pop, 3 of 4\\n– Who Is It, Michael Jackson, Dangerous, Pop (Deluxe), 3\\nof 4\\n– Forget Me, Lewis Capaldi, Forget Me, Pop Pop, 2022, 3\\nof 4\\n– Pop, Speak Now, Taylor Swift, 2014, (Deluxe), 3 of 4\\n– Pop Pop, Maroon 5, Overexposed, 2016, 3 of 4\\n– Pointless, Lewis Capaldi, Pointless, Pop, 2022, 3 of 4\\n– Saved, Khalid, American Teen, Pop, 2022, 3 of 4\\n– Deja vu, Fearless, Pop, 2020, (Deluxe), 3 of 4\\nTable 7: Text prompts composed for the four common\\nmusic genres: electronic, hip hop, metal, and pop.Prompt\\n– Aerials, System Of A Down, Toxicity, 2001, 2 of 4\\n– Aloo Gobi, Weezer, OK Human, 2021, 1 of 4\\n– Bananas and Blow, Ween, White Pepper, 3 of 4\\n– Blue Light, Bloc Party, Silent Alarm, 2005, 1 of 4\\n– Break-Thru, Dirty Projectors, Lamp Lit Prose, 2018, 3 of\\n4\\n– B:/ Start Up, Blank Banshee, Blank Banshee 0, Future\\nFunk, 4 of 4\\n– Carrion Crawler, Thee Oh Sees, Carrion Crawler / The\\nDream, 2011, 4 of 4\\n– Change, Tears For Fears, The Hurting, 1983, 3 of 4\\n– Comic, Bo Burnham, INSIDE (DELUXE), 4 of 4\\n– Eaten by Worms, Nothing, 2016, 2 of 4\\n– Effect and Cause, The White Stripes, Icky Thump, 2007, 4\\nof 4\\n– Feeling Good, Muse, 2001, 4 of 4\\n– Hip Hop, Tyler, The Creator, IGOR, 2019, 3 of 4\\n– Ice, The Microphones, It Was Hot, We Stayed in the Water,\\n2000, 2 of 4\\n– In My Pocket, Temples, 2 of 4\\n– Liquid State, Muse, 2012, 4 of 4\\n– Moonlight, Death From Above 1979, Outrage! Is Now,\\n2017, 2 of 4\\n– Mutilated Lips, Ween, The Mollusk, 1997, 2 of 4\\n– My Kind of Woman, Mac DeMarco, 2, 2012, 2 of 4\\n– Night Shop, Optiganally Yours, O.Y . in Hi-Fi, 2018, 3 of 4\\n– Red Eye Flashes Twice, Jeffery Dallas, 2010, 3 of 4\\n– Reflektor, Arcade Fire, Reflektor, 2013, 2 of 4\\n– Some Thing’s Coming, I Monster, Neveroddoreven, 2005,\\n3 of 4\\n– Spaceship, Kanye West/GLC/Consequence, The College\\nDropout, 1 of 4\\n– Superfast Jellyfish (feat. Gruff Rhys and De La Soul),\\nGorillaz/Gruff Rhys/De La Soul, Plastic Beach, 2010, 2 of 4\\n– The Well and the Lighthouse, Arcade Fire, Neon Bible,\\n2007, 4 of 4\\n– Well, You Can Do It Without Me, Fear Fun, 4 of 4\\n– Upgrade (A Baymar College College), Deltron 3030/Del\\nThe Funky Homosapien/Dan The Automator/Kid Koala, Del-\\ntron 3030, 2000, 4 of 4\\n– Way We Won’t, Grandaddy, Last Place, 2017, 3 of 4\\n– Safe From Heartbreak (if you never fall in love), Wolf\\nAlice, 2 of 4\\n– Black Alternative Metal, The Pick of Death (Deluxe), 2006,\\n3 of 4\\n– Death Metal, 2012, 3 of 4\\n– Drops, Kanine Remix, Darkzy, Drops Remixes, bass house,\\n(Deluxe) (Remix), 3 of 4\\n– EDM (Deluxe) (Remix), 3 of 4\\n– Electro House (Remix), 2023, 3 of 4\\n– Electro Swing Remix 2030 (Deluxe Edition), 3 of 4\\n– Future Bass, EDM (Remix), Remix, 3 of 4\\n– Hip Hop Tech, Bandlez, Hot Pursuit, brostep, 3 of 4\\n– Italian Hip Hop 2022 (Deluxe Edition), 3 of 4\\n– Heavy metal (Deluxe Edition), 3 of 4\\n– The Heavy Death Metal War (Deluxe), 2006, 3 of 4\\n– Pop, Taylor Swift, Speak Now, 2014, (Deluxe), 3 of 4\\n– Melodic Metal, Iron Dust (Deluxe), 2006, 3 of 4\\n– Electronic, Dance, EDM (Deluxe) (Remix), 3 of 4\\n– Alternative Hip Hop Oh-My, 2016, (Deluxe), 3 of 4\\n– Viking Heavy Death Metal (Deluxe), 2006, 3 of 4\\n– Possessed Death Metal Stones (Deluxe), 2006, 3 of 4\\n– Hardstyle, Drop, 8D, Remix, High Quality, 2 of 4\\n– Drop, French 79, BPM Artist, V ol. 4, Electronica, 2016\\n– Dubstep Insane Drop Remix (Deluxe Edition), 2 of 4\\nTable 8: Text prompts used to generate music for the\\nmusicality test.', metadata={'source': '/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs/Text-to-Music Generation.pdf', 'page': 18}),\n"," Document(page_content=\"11/27/23, 3:23 PM Transforming Machine Learning Models into APIs in Python | by Asad iqbal | Medium\\nhttps://medium.com/@deasadiqbal/transforming-machine-learning-models-into-apis-in-python-f9af614ab499 3/151 — One approach is to consider rewriting the entire ML codebase in a language\\nthat suits the software engineering team. While this might seem like a logical\\nsolution, it can be incredibly time-consuming and resource-intensive, especially\\nwhen dealing with complex models. Additionally, some languages, such as\\nJavaScript, lack robust ML libraries, making this option less appealing.\\n2 — Another, more practical approach is to adopt an API-first strategy. Web APIs\\nhave revolutionized the way applications interact across languages. By exposing\\nyour ML models through a web API, you provide a seamless bridge between data\\nscience and software engineering. Frontend developers, for example, can access\\nyour ML model via a simple URL endpoint, enabling them to integrate machine\\nlearning capabilities into web applications effortlessly.\\nBefore delving deeper into the API-first approach, let’s take a moment to\\nunderstand what an API really is and how it can serve as a powerful\\nconnector between different technology stacks.\\nW hat are APIs?\\n“In simple words, an API is a (hypothetical) contract between 2 software's\\nsaying if the user software provides input in a pre-defined format, the later\\nwith extend its functionality and provide the outcome to the user software.”\\nEssentially, APIs function much like web applications, though with a distinct\\nfocus on data exchange rather than presenting information in a user-friendly\\nHTML format. Instead, they typically return data in standardized formats\\nlike JSON or XML. Once a developer obtains the desired output from an API,\\nthey have the creative freedom to style and present it according to their\\nspecific preferences and requirements. This flexibility empowers developers\\nto integrate data seamlessly into their applications while maintaining full\", metadata={'source': '/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs/APIs.pdf', 'page': 0}),\n"," Document(page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2', metadata={'source': '/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs/Attention is all You need.pdf', 'page': 0}),\n"," Document(page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .\\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3', metadata={'source': '/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs/Attention is all You need.pdf', 'page': 1}),\n"," Document(page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4', metadata={'source': '/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs/Attention is all You need.pdf', 'page': 2}),\n"," Document(page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\\n5', metadata={'source': '/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs/Attention is all You need.pdf', 'page': 3}),\n"," Document(page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6', metadata={'source': '/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs/Attention is all You need.pdf', 'page': 4}),\n"," Document(page_content='length nis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate =d−0.5\\nmodel·min(step_num−0.5, step _num·warmup _steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup _steps = 4000 .\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7', metadata={'source': '/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs/Attention is all You need.pdf', 'page': 5})]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["documents"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["We have, 119 chunks in memory\n"]}],"source":["text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE,\n","                                                chunk_overlap=100)\n","\n","splits = text_splitter.split_documents(documents)\n","\n","# length of all splits\n","\n","print(f\"We have, {len(splits)} chunks in memory\")"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["vectorstore_db = FAISS.from_documents(splits, embeddings)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["retriever = vectorstore_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Retrieved documents: 6\n"]},{"data":{"text/plain":["'Page content of first document:\\n Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .'"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["retrieved_relevant_docs = retriever.get_relevant_documents(\n","    \"What is a Transformer?\"\n",")\n","\n","print(f\"Retrieved documents: {len(retrieved_relevant_docs)}\")\n","f\"Page content of first document:\\n {retrieved_relevant_docs[0].page_content}\""]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Retrieved documents: 6\n"]},{"data":{"text/plain":["'Page content of first document:\\n repetitiveness of the melody. A more rhyth-\\nmic and repetitive melody is considered better,\\nwhile the opposite is true for a less rhythmic\\nmelody.\\n7.Harmoniousness involves multiple notes\\nplayed together to support the melody. Evalu-\\nate if these notes are in sync and enhance the\\nFigure 7: Mel spectrogram comparison between the true\\nsamples (top) and the auto-encoded samples (bottom);\\ncf. text.\\neffect of the melody. Higher scores should be\\ngiven for good harmony and lower for poor\\nharmony.\\n8.It is recommended view youtube videos: this\\nor this short video explaining melody and har-\\nmony\\n9.This folder also contains a spreadsheet by the\\nname “Response_Task_1.xlsx”. Remember\\nto provide ratings (out of 5) for each aspect\\nof your evaluation in the file against appropri-\\nate folder number. Feel free to listen to each\\nsample as many times before rating them.\\nD Exploring Variations of the Model\\nArchitecture and Training Setup\\nD.1 High-Frequency Sounds'"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["retrieved_relevant_docs = retriever.get_relevant_documents(\n","    \"What is a melody?\"\n",")\n","\n","print(f\"Retrieved documents: {len(retrieved_relevant_docs)}\")\n","f\"Page content of first document:\\n {retrieved_relevant_docs[0].page_content}\""]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["custom_prompt_template = \"\"\"You are an assistant for question-answering tasks. Use the uploaded files to answer the question at the end.\n","If you don't know the answer just say you do not know and do not try to make up the answer. Keep the answer as concise as possible.\n","Context= {context}\n","History = {history}\n","Question= {question}\n","Helpful Answer:\n","\"\"\"\n","\n","prompt = PromptTemplate(template=custom_prompt_template,\n","                        input_variables=[\"question\", \"context\", \"history\"])"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.09s/it]\n"]}],"source":["qa_chain_with_memory = RetrievalQA.from_chain_type(llm=load_llm(), chain_type='stuff',\n","                                                   retriever = vectorstore_db.as_retriever(),\n","                                                   return_source_documents = True,\n","                                                   chain_type_kwargs = {\"prompt\": prompt,\n","                                                                        \"memory\": ConversationBufferMemory(\n","                                                                            input_key=\"question\",\n","                                                                            memory_key=\"history\",\n","                                                                            return_messages=True)})"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"data":{"text/plain":["'The decoder is the part of the Transformer model that generates the answer to the question. It is composed of an encoder and a decoder.\\nThe encoder is responsible for encoding the question into a representation that can be used by the'"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["query = \"What is a decoder in transformers?\"\n","qa_chain_with_memory.run({\"query\": query})"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"data":{"text/plain":["'1. Encoder\\n2. Decoder\\n3. Self-Attention\\n4. Multi-Head Attention\\n5. Residual Connection\\n6. Layer Normalization\\n7. Fully Connected Layers\\n'"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["qa_chain_with_memory({\"query\": \"What are its components?\"})[\"result\"]"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"data":{"text/plain":["{'query': 'What is a chair',\n"," 'result': 'A chair is a piece of furniture with a flat bottom and raised sides, used for sitting.',\n"," 'source_documents': [Document(page_content='the Eleventh International Conference on Language Re-\\nsources and Evaluation (LREC 2018) , Miyazaki, Japan.\\nEuropean Language Resources Association (ELRA).\\nJeanette Bicknell. 2002. Can music convey semantic\\ncontent? a kantian approach. The Journal of Aesthetics\\nand Art Criticism , 60(3):253–261.\\nZalán Borsos, Raphaël Marinier, Damien Vincent,\\nEugene Kharitonov, Olivier Pietquin, Matthew Shar-\\nifi, Olivier Teboul, David Grangier, Marco Tagliasac-\\nchi, and Neil Zeghidour. 2022. AudioLM: A lan-\\nguage modeling approach to audio generation. CoRR ,\\nabs/2209.03143.\\nNicolas Boulanger-Lewandowski, Yoshua Bengio, and\\nPascal Vincent. 2012. Modeling temporal dependencies\\nin high-dimensional sequences: Application to poly-\\nphonic music generation and transcription.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen', metadata={'source': '/home/brianmutea/Gradio-app-Chat-with-Multiple-PDFs-using-LangChain-an-DeciLM-6b-instruct/PDFs/Text-to-Music Generation.pdf', 'page': 10})]}"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["qa_chain_with_memory({\"query\": \"What is a chair\"})"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":2}
